 
## üéØ ¬øPara qu√© sirve hacer r√©plicas en PostgreSQL?

### ‚úÖ 1. **Alta disponibilidad (High Availability)**
- La replicaci√≥n permite que una base de datos est√© disponible incluso si el servidor principal falla. Los datos se copian a uno o m√°s servidores de r√©plica, que pueden asumir el rol del servidor principal en caso de fallo.


### ‚úÖ 2. **Balanceo de carga (Load Balancing)**
- Las r√©plicas pueden ser utilizadas para distribuir la carga de trabajo, permitiendo que las consultas de solo lectura se ejecuten en los servidores de r√©plica, aliviando la carga del servidor principal.

### ‚úÖ 3. **Recuperaci√≥n ante desastres (Disaster Recovery)**
- En caso de un desastre, las r√©plicas pueden ser utilizadas para restaurar r√°pidamente los datos y minimizar el tiempo de inactividad.

### ‚úÖ 4. **An√°lisis sin afectar producci√≥n**
- Puedes hacer an√°lisis pesados o pruebas en una r√©plica sin afectar el rendimiento del servidor principal.

### ‚úÖ 5. **Migraciones o actualizaciones**
- Puedes usar una r√©plica l√≥gica para migrar datos entre versiones diferentes de PostgreSQL o hacia otro sistema.

### ‚úÖ 6. **Integraci√≥n con otros sistemas**
- La replicaci√≥n l√≥gica permite enviar cambios en tiempo real a sistemas como Kafka, Elasticsearch, BigQuery, etc.

---

## üîÑ ¬øCu√°ndo usar cada tipo?

| Objetivo | Tipo de r√©plica recomendada | ¬øPor qu√©? |
|---------|------------------------------|-----------|
| Alta disponibilidad | **Streaming (f√≠sica)** | R√©plica exacta del servidor, lista para tomar el control. |
| Balanceo de carga | **Streaming (f√≠sica)** | Ideal para consultas de solo lectura. |
| An√°lisis o BI | **L√≥gica** | Puedes replicar solo ciertas tablas. |
| Integraci√≥n con otros sistemas | **L√≥gica** | Permite enviar cambios en formato JSON o eventos. |
| Migraci√≥n entre versiones | **L√≥gica** | Compatible entre versiones distintas. |

### üîç Comparaci√≥n: Replicaci√≥n en Streaming vs. Replicaci√≥n L√≥gica

 

| **Caracter√≠stica**             | **Replicaci√≥n en Streaming (F√≠sica)**                              | **Replicaci√≥n L√≥gica**                                                                 |
|-------------------------------|---------------------------------------------------------------------|----------------------------------------------------------------------------------------|
| **Nivel de replicaci√≥n**      | A nivel de bloque de disco (f√≠sico)                                | A nivel de cambios en filas y tablas (l√≥gico)                                         |
| **Uso del WAL**               | Se transmite tal cual, binario                                     | Se decodifica el WAL en cambios l√≥gicos (INSERT, UPDATE, DELETE)                     |
| **Requiere estructura id√©ntica** | S√≠ (mismo esquema, extensiones, etc.)                              | No necesariamente (puede haber diferencias en tablas, columnas, etc.)                |
| **Flexibilidad**              | Limitada                                                           | Alta (puedes replicar solo algunas tablas, transformar datos, etc.)                  |
| **Casos de uso**              | Alta disponibilidad, failover                                      | Integraci√≥n, migraciones, replicaci√≥n parcial, multi-master (con herramientas externas) |
| **Transmisi√≥n del WAL**       | Se transmite el WAL completo en formato binario a cada r√©plica     | Cada suscriptor recibe solo los cambios relevantes del publicar, y ya decodificados desde el WAL usando el plugin l√≥gico (pgoutput, wal2json, etc.)       |
 

---
 
### üîπ **Failover**
El **failover** ocurre cuando el **nodo primario** falla inesperadamente y un nodo **standby** se convierte autom√°ticamente en el nuevo **nodo primario**.  
‚úÖ Se activa de manera autom√°tica en sistemas configurados con monitoreo y failover.  
‚úÖ Evita la ca√≠da total del servicio.  
‚úÖ Se usa en escenarios de emergencia cuando el servidor principal deja de funcionar.  
Ejemplo de comando manual:
 
### üîπ **Switchover**
El **switchover** es un proceso planificado en el que el nodo **primario** y uno **standby** intercambian roles de forma controlada.  
‚úÖ Se realiza sin que haya fallos en el sistema.  
‚úÖ Se usa para mantenimiento o actualizaciones del nodo primario.  
‚úÖ Permite cambiar de l√≠der sin interrupciones.  
 

üí° **Diferencia clave:**  
- **Failover** = Evento inesperado, ocurre por una falla.  
- **Switchover** = Cambio intencional y programado.


---

## ‚ö†Ô∏è Consideraciones

- La **replicaci√≥n f√≠sica** es m√°s simple y r√°pida, pero menos flexible.
- La **replicaci√≥n l√≥gica** es m√°s flexible (puedes elegir tablas), pero requiere m√°s configuraci√≥n.
- Ambas pueden coexistir si configuras `wal_level = logical`.

 

## üîç Niveles de `wal_level` en PostgreSQL

### 1. `minimal`
- **¬øQu√© hace?**  
  Registra solo lo estrictamente necesario para la recuperaci√≥n ante fallos.
- **Uso t√≠pico:**  
  Operaciones de carga masiva (`COPY`, `INSERT`) cuando no se necesita replicaci√≥n ni puntos de recuperaci√≥n avanzados.
- **Ventajas:**  
  - Menor volumen de WAL.
  - Mejor rendimiento en operaciones masivas.
- **Desventajas:**  
  - No permite replicaci√≥n.
  - No se puede usar para backups PITR (Point-In-Time Recovery).

---

### 2. `replica` (antes llamado `archive`)
- **¬øQu√© hace?**  
  Registra suficiente informaci√≥n para replicaci√≥n f√≠sica y recuperaci√≥n PITR.
- **Uso t√≠pico:**  
  Replicaci√≥n f√≠sica entre servidores PostgreSQL (standby).
- **Ventajas:**  
  - Permite replicaci√≥n f√≠sica.
  - Compatible con herramientas de backup como `pg_basebackup`.
- **Desventajas:**  
  - No permite replicaci√≥n l√≥gica.
  - M√°s volumen de WAL que `minimal`.

---

### 3. `logical`
- **¬øQu√© hace?**  
  Registra todos los cambios necesarios para replicaci√≥n l√≥gica (como `wal2json`, `pgoutput`, etc.).
- **Uso t√≠pico:**  
  - Replicaci√≥n l√≥gica.
  - Integraci√≥n con sistemas externos (Kafka, Debezium, etc.).
  - Captura de datos en cambio (CDC).
- **Ventajas:**  
  - Permite replicaci√≥n l√≥gica y f√≠sica.
  - Ideal para arquitecturas orientadas a eventos.
- **Desventajas:**  
  - Mayor volumen de WAL.
  - Puede impactar el rendimiento si no se gestiona bien.

 

# Conceptos que se usan en las replicas 
```sql

Maestro/Primario
Esclavo/secundario/standby

- **Activo-Activo**:  En PostgreSQL 16, se ha mejorado la replicaci√≥n l√≥gica para permitir una configuraci√≥n Activo-Activo, donde dos instancias de PostgreSQL pueden recibir escrituras simult√°neamente y sincronizar los cambios entre ellas.
En este modelo, todos los nodos est√°n operativos y procesan solicitudes y cambios simult√°neamente. Esto permite distribuir la carga de trabajo entre m√∫ltiples servidores, mejorando el rendimiento y la disponibilidad. Si un nodo falla, los dem√°s contin√∫an funcionando sin interrupciones.

- **Activo-Pasivo**: Aqu√≠, solo un nodo est√° activo y maneja las solicitudes, mientras que otro nodo permanece en espera (pasivo). Si el nodo activo falla, el pasivo puede toma el control mediante failover. Este enfoque es m√°s simple y garantiza estabilidad, pero no aprovecha los recursos del nodo pasivo hasta que sea necesario.

El **split-brain** es un problema que ocurre en sistemas de alta disponibilidad y replicaci√≥n, cuando dos nodos **pierden comunicaci√≥n entre s√≠**, pero **ambos creen que son el primario** al mismo tiempo.
  
----------------------------------------------------------------------------------------------------------------------------------------------------------------

¬øQu√© significa el consenso en t√©rminos generales?  Es un acuerdo entre m√∫ltiples participantes ‚Üí Un grupo debe tomar una decisi√≥n colectiva basada en reglas claras.  Evita que haya decisiones individuales incorrectas ‚Üí Por ejemplo, en replicaci√≥n de bases de datos, un nodo no puede decidir solo convertirse en primario sin confirmaci√≥n de los dem√°s.  Se usa en algoritmos de failover y gesti√≥n de sistemas distribuidos ‚Üí Como Raft, Paxos y Etcd, que permiten que los servidores acuerden qui√©n es el l√≠der.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

CAP Theorem ‚Üí En bases de datos distribuidas, puedes tener Consistencia (C), Disponibilidad (A) o Tolerancia a Particiones (P), pero nunca las tres simult√°neamente.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

Un Data Warehouse es un sistema de almacenamiento y gesti√≥n de datos dise√±ado para facilitar el an√°lisis y la toma de decisiones en una organizaci√≥n. Funciona como un repositorio central donde se integran y estructuran grandes vol√∫menes de informaci√≥n provenientes de m√∫ltiples fuentes.

```

## **Raft** y **Paxos** 
 son algoritmos de *consenso distribuido*, dise√±ados para que m√∫ltiples nodos en un sistema lleguen a un acuerdo sobre un valor, incluso si algunos fallan o se desconectan. Son fundamentales en bases de datos distribuidas, sistemas de archivos y cl√∫steres de alta disponibilidad.


###  **¬øQu√© es Paxos?**

- Propuesto por Leslie Lamport en los a√±os 80.
- Es un algoritmo robusto pero **dif√≠cil de entender e implementar**.
- Usa tres roles:
  - **Proposers**: proponen valores.
  - **Acceptors**: aceptan o rechazan propuestas.
  - **Learners**: aprenden el valor acordado.
- Funciona por medio de rondas de propuestas y promesas, buscando que **una mayor√≠a (qu√≥rum)** acepte un valor.

 *Ventaja:* muy tolerante a fallos.  
 *Desventaja:* complejo, propenso a errores de implementaci√≥n.


###Ô∏è **¬øQu√© es Raft?**

- Dise√±ado en 2013 por Diego Ongaro y John Ousterhout como una alternativa m√°s **entendible** a Paxos.
- Usa un enfoque **basado en liderazgo**:
  - Un nodo es elegido como **l√≠der**.
  - Los dem√°s son **seguidores**.
  - Si el l√≠der falla, se realiza una **elecci√≥n** para elegir uno nuevo.
- El l√≠der recibe las operaciones y las **replica en todos los nodos**.

 *Ventaja:* m√°s f√°cil de implementar y razonar.  
*Desventaja:* no es tolerante a fallos bizantinos (no protege contra nodos maliciosos).


---

### üîë ¬øQu√© es el qu√≥rum?

Es **la mayor√≠a necesaria de nodos de consenso (por ejemplo, etcd)** que deben estar **activos y en acuerdo** para que se puedan tomar decisiones cr√≠ticas de manera segura.  
Ejemplo cl√°sico: en un cl√∫ster de 3 nodos etcd, **se necesita al menos 2 funcionando** para tener qu√≥rum. Herramientas como etcd exige quorum y si no hay mayor√≠a (quorum) de nodos disponibles no aceptar√° escrituras ni permitir√° elecciones de l√≠der 

### üìå Reglas clave:

- El **qu√≥rum se calcula sobre los nodos consenso como etcd**, **no sobre los servidores PostgreSQL**.
- Siempre necesitas al menos **una mayor√≠a de nodos etcd funcionales** para que Patroni pueda tomar decisiones cr√≠ticas como un failover.
- **Debe ser siempre un n√∫mero impar** para facilitar la mayor√≠a.
- F√≥rmula: Para tolerar _f_ fallos ‚Üí necesitas **2√óf + 1** nodos etcd.


### üß† ¬øPor qu√© es tan importante?

Porque sin qu√≥rum:

- No se puede promover una r√©plica a primario.
- El sistema entra en estado de seguridad (failover bloqueado).
- Se evita el *split-brain* (dos nodos crey√©ndose l√≠deres al mismo tiempo).


### Configuraci√≥n de nodos etcd y tolerancia a fallos

| # Nodos etcd | Qu√≥rum necesario | Fallos tolerables | ¬øCu√°ndo usarlo?                              |
|--------------|------------------|-------------------|----------------------------------------------|
| 1 (no recomendado) | 1                | 0                 | Solo para pruebas locales ‚Äì ‚ùå Punto √∫nico de fallo |
| 3 (ideal m√≠nimo)   | 2                | 1                 | Producci√≥n b√°sica                            |
| 5                 | 3                | 2                 | Alta disponibilidad en m√∫ltiples zonas       |
| 7                 | 4                | 3                 | Infraestructura cr√≠tica o multinube          |
 
  
### üß† **¬øCu√°ndo deber√≠as considerar aumentar los nodos de consenso?**
Saber cu√°ndo aumentar el n√∫mero de nodos en tu cl√∫ster de consenso (como etcd) no depende de cu√°ntos servidores PostgreSQL tengas, sino de cu√°nto Fallos de consenso est√°s dispuesto a tolerar y qu√© tan cr√≠tica es tu infraestructura y en un sistema donde se prestan fallos comunes.

1. **Cuando necesitas tolerar m√°s fallos**
   - Si actualmente tienes 3 nodos etcd, solo puedes tolerar 1 ca√≠da.
   - Si quieres tolerar 2 fallos simult√°neos, necesitas 5 nodos.

 ---

# Conocimiento esencial para dise√±ar arquitecturas distribuidas eficientes.  

- Ley de Amdahl
- Ley de Gunther
- F√≥rmula de latencia en redes distribuidas
- F√≥rmula de Throughput
- F√≥rmula de Consistencia CAP
- Teorema de Brewer (PACELC)
- Ley de Little
- F√≥rmula de escalabilidad de Gustafson
---

### **üìå Ley de Amdahl ‚Äì L√≠mite de aceleraci√≥n en paralelizaci√≥n**  

üìç **F√≥rmula general**  

$$  S = \frac{1}{(1 - P) + \frac{P}{N}}  $$

üìç **Significado de cada variable**  
- **S (Speedup)** ‚Üí **Variable**: Es el resultado final de cu√°nto mejora el rendimiento del sistema.  
- **P (Parallelizable Fraction)** ‚Üí **Variable**: Porcentaje del sistema que puede ejecutarse en paralelo.  
- **N (Number of Processors)** ‚Üí **Variable**: Cantidad de nodos o procesadores usados en paralelo.  
- **(1 - P)** ‚Üí **Constante**: Representa la parte del sistema que **siempre ser√° secuencial** y no se puede paralelizar.  

üìç **Ejemplo pr√°ctico**  
Supongamos que queremos procesar un conjunto de datos en PostgreSQL:  
- **El 80% de la tarea puede ejecutarse en paralelo** (`P = 0.8`).  
- **Usaremos 4 servidores** (`N = 4`).  

Aplicamos la f√≥rmula:  

$$ S = \frac{1}{(1 - 0.8) + \frac{0.8}{4}} $$  

$$ S = \frac{1}{0.2 + 0.2} $$

$$ S = \frac{1}{0.4} = 2.5 $$

üìå **Conclusi√≥n**  
Aunque agreguemos **4 nodos**, el sistema solo se vuelve **2.5 veces m√°s r√°pido**, porque a√∫n hay una fracci√≥n **(1 - P)** que nunca podr√° paralelizarse. Este principio es clave en sistemas distribuidos: m√°s servidores no siempre significan m√°s velocidad.

---

### **üìå Ley de Gunther ‚Äì L√≠mite de escalabilidad en un sistema**  

üìç **F√≥rmula general**  

$$ X(N) = \frac{N}{1 + \sigma (N - 1)} $$

üìç **Significado de cada variable**  
- **X(N) (Rendimiento escalado)** ‚Üí **Variable**: Resultado final de cu√°nto mejora el rendimiento real del sistema con `N` nodos.  
- **N (Number of Nodes)** ‚Üí **Variable**: Cantidad de servidores en el sistema distribuido.  
- **œÉ (Contention Factor)** ‚Üí **Variable**: Porcentaje de contenci√≥n por recursos compartidos en el sistema.  
- **El n√∫mero "1" en el denominador** ‚Üí **Constante**: Representa la ejecuci√≥n sin contenci√≥n.  

üìç **Ejemplo pr√°ctico**  
Supongamos que queremos **ampliar un cl√∫ster de bases de datos** con Citus:  
- **Tenemos 10 nodos** (`N = 10`).  
- **La contenci√≥n causada por comunicaci√≥n es 30%** (`œÉ = 0.3`).  

Aplicamos la f√≥rmula:  

$$ X(10) = \frac{10}{1 + 0.3 (10 - 1)} $$

$$ X(10) = \frac{10}{1 + 2.7} $$

$$ X(10) = \frac{10}{3.7} = 2.7 $$

üìå **Conclusi√≥n**  
Aunque agregamos **10 nodos**, el rendimiento **solo se multiplica por 2.7** debido a la contenci√≥n de recursos compartidos. Esto demuestra que simplemente agregar m√°s servidores no siempre es la mejor estrategia sin optimizaci√≥n.

---

### **üìå F√≥rmula de latencia en redes distribuidas**  

üìç **F√≥rmula general**  

$$ L = RTT + \frac{S}{B} $$

üìç **Significado de cada variable**  
- **L (Latency)** ‚Üí **Variable**: Tiempo total que tarda una operaci√≥n en completarse en el sistema distribuido.  
- **RTT (Round Trip Time)** ‚Üí **Constante**: Tiempo de ida y vuelta de los paquetes en la red.  
- **S (Size of Message)** ‚Üí **Variable**: Tama√±o del dato que se transmite en la red.  
- **B (Bandwidth)** ‚Üí **Variable**: Velocidad de transmisi√≥n de la red (Mbps).  

üìç **Ejemplo pr√°ctico**  
Si tenemos una conexi√≥n donde:  
- **RTT es 50 ms** (`RTT = 50`).  
- **El mensaje tiene 5 MB** (`S = 5000 KB`).  
- **El ancho de banda es 100 Mbps** (`B = 100000 KB/s`).  

Aplicamos la f√≥rmula:  

$$ L = 50 + \frac{5000}{100000} $$

$$ L = 50 + 0.05 = 50.05 ms $$

üìå **Conclusi√≥n**  
La latencia total es **50.05 ms**, y lo que m√°s afecta el rendimiento es el **RTT**, que es una constante del sistema. Aunque se aumente el ancho de banda, el tiempo m√≠nimo de ida y vuelta **siempre ser√° 50 ms**.


 
---
 

### **üìå F√≥rmula de Throughput ‚Äì Capacidad del sistema para procesar operaciones**  

üìç **F√≥rmula general**  

$$ T = \frac{N}{L} $$  

üìç **Significado de cada variable**  
- **T (Throughput)** ‚Üí **Variable**: Indica cu√°ntas operaciones por segundo puede manejar el sistema.  
- **N (Number of Transactions)** ‚Üí **Variable**: Es la cantidad total de operaciones que el sistema debe procesar.  
- **L (Latency per Transaction)** ‚Üí **Variable**: Tiempo que toma cada operaci√≥n en completarse.  

üìç **Valores constantes:**  
‚úÖ **La estructura de la ecuaci√≥n** ‚Üí Siempre ser√° una **divisi√≥n entre cantidad de operaciones y su latencia**, ya que el concepto de rendimiento no cambia.  

üìç **Ejemplo pr√°ctico**  
Imaginemos que tenemos un sistema distribuido con **10,000 operaciones** (`N = 10,000`) y cada transacci√≥n tarda **500 ms** (`L = 0.5 segundos`). Aplicamos la f√≥rmula:  

$$ T = \frac{10,000}{0.5} $$  

$$ T = 20,000 \text{ operaciones/segundo} $$  

üìå **Conclusi√≥n**  
Este sistema es capaz de procesar **20,000 operaciones por segundo**. Si queremos mejorar el rendimiento, podemos:  
- **Reducir la latencia (`L`)** optimizando consultas.  
- **Aumentar la cantidad de nodos** para procesar m√°s transacciones en paralelo.  


---
### **üìå F√≥rmula de Consistencia CAP ‚Äì Equilibrio en sistemas distribuidos**  

üìç **F√≥rmula general**  
El **Teorema CAP** establece que un sistema distribuido **puede garantizar solo dos de tres propiedades**:  

$$  C + A + P \neq 3 $$  

Donde:  
- **C (Consistency)** ‚Üí **Variable**: Garantiza que todos los nodos ven los mismos datos al mismo tiempo.  
- **A (Availability)** ‚Üí **Variable**: Asegura que cada solicitud recibe una respuesta, incluso si algunos nodos fallan.  
- **P (Partition Tolerance)** ‚Üí **Constante**: El sistema sigue funcionando a pesar de fallos en la red.  

üìç **Ejemplo pr√°ctico**  
Supongamos que tenemos una base de datos distribuida y ocurre una **falla de red**.  
- Si priorizamos **Consistencia (C) y Partici√≥n (P)**, el sistema **rechazar√° algunas solicitudes** para garantizar datos correctos.  
- Si priorizamos **Disponibilidad (A) y Partici√≥n (P)**, el sistema **seguir√° respondiendo**, pero algunos datos pueden estar desactualizados.  

üìå **Conclusi√≥n**  
No es posible tener **las tres propiedades al mismo tiempo**. Cada sistema debe elegir entre **CP (consistencia y tolerancia a fallos)** o **AP (disponibilidad y tolerancia a fallos)** seg√∫n sus necesidades.  

---
### **üìå Teorema de Brewer (PACELC) ‚Äì Extensi√≥n del CAP Theorem**  
El **PACELC Theorem** es una extensi√≥n del **CAP Theorem**, que introduce un nuevo concepto clave en sistemas distribuidos: **latencia**.  

üìå **¬øQu√© significa PACELC?**  
El acr√≥nimo representa:  
- **P** ‚Üí **Partitioning** (Partici√≥n en la red).  
- **A** ‚Üí **Availability** (Disponibilidad).  
- **C** ‚Üí **Consistency** (Consistencia).  
- **E** ‚Üí **Else** (Si no hay partici√≥n).  
- **L** ‚Üí **Latency** (Latencia).  
- **C** ‚Üí **Consistency** (Consistencia).  

üìç **F√≥rmula conceptual**  
Si hay **partici√≥n en la red**, el sistema debe elegir entre **Consistencia (C) o Disponibilidad (A)**.  
Si **no hay partici√≥n**, el sistema debe elegir entre **Latencia baja (L) o Consistencia (C)**.  

üìå **Importancia:**  
Este teorema ampl√≠a el **CAP Theorem**, agregando la dimensi√≥n de **latencia** en sistemas distribuidos.  

üìå **¬øC√≥mo funciona PACELC?**  
El teorema establece dos escenarios:  
1. **Si hay una partici√≥n en la red (P)** ‚Üí Se debe elegir entre **Disponibilidad (A) o Consistencia (C)** (como en el CAP Theorem).  
2. **Si no hay partici√≥n (Else - E)** ‚Üí Se debe elegir entre **Latencia baja (L) o Consistencia (C)**.  

üìå **Ejemplo pr√°ctico**  
- **Google Spanner** prioriza **consistencia** en todo momento (**PC/EC**).  
- **Cassandra** prioriza **disponibilidad y baja latencia** (**PA/EL**).  

üìå **Importancia**  
PACELC mejora el CAP Theorem al considerar **rendimiento y latencia**, lo que es crucial en bases de datos distribuidas y aplicaciones en la nube.  

üìå **D√≥nde se usa:**  
- Dise√±o de **bases de datos distribuidas** como Cassandra, Spanner y Citus.  
- Evaluaci√≥n de **arquitecturas de microservicios**.  
- Optimizaci√≥n de **sistemas de almacenamiento en la nube**.  

--- 
 
### **üìå Ley de Little ‚Äì Relaci√≥n entre tiempo de respuesta y concurrencia**  
üìç **F√≥rmula general**  

$$ L = \lambda W $$
 
üìç **Significado de cada variable**  
- **L (Longitud de la cola)** ‚Üí **Variable**: N√∫mero promedio de solicitudes en espera en el sistema.  
- **Œª (Tasa de llegada)** ‚Üí **Variable**: Cantidad de solicitudes que llegan por unidad de tiempo.  
- **W (Tiempo de espera promedio)** ‚Üí **Variable**: Tiempo que cada solicitud pasa en el sistema.  

üìå **Importancia:**  
Ayuda a calcular **cu√°nto tr√°fico puede manejar un sistema distribuido** antes de que se vuelva lento.  

üìå **D√≥nde se usa:**  
- Dise√±o de **balanceo de carga** en servidores.  
- Optimizaci√≥n de **colas de procesamiento** en bases de datos.  
- Evaluaci√≥n de **rendimiento en APIs** y sistemas web.  

---

 
### **üìå F√≥rmula de escalabilidad de Gustafson ‚Äì Correcci√≥n de la Ley de Amdahl**  
üìç **F√≥rmula general**  

$$  S = N - (1 - P) (N - 1) $$  

üìç **Significado de cada variable**  
- **S (Speedup)** ‚Üí **Variable**: Aceleraci√≥n del sistema con paralelizaci√≥n.  
- **N (Number of Processors)** ‚Üí **Variable**: N√∫mero de nodos o procesadores usados.  
- **P (Parallelizable Fraction)** ‚Üí **Variable**: Porcentaje del sistema que puede ejecutarse en paralelo.  

üìå **Importancia:**  
Corrige la **Ley de Amdahl**, mostrando que **m√°s nodos pueden mejorar el rendimiento** si el problema se escala correctamente.  

üìå **D√≥nde se usa:**  
- Dise√±o de **clusters de computaci√≥n distribuida**.  
- Optimizaci√≥n de **procesamiento en paralelo** en bases de datos.  
- Evaluaci√≥n de **rendimiento en sistemas de Big Data**.  


---


 

## Bibliograf√≠a 
```
https://www.youtube.com/watch?v=kW8xT_cgEMM
https://medium.com/@c.ucanefe/patroni-ha-proxy-feed1292d23f


https://www.geeksforgeeks.org/paxos-vs-raft-algorithm-in-distributed-systems/
https://dev.to/pragyasapkota/consensus-algorithms-paxos-and-raft-37ab

```
