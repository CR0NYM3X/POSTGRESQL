 
## ğŸ¯ Â¿Para quÃ© sirve hacer rÃ©plicas en PostgreSQL?

### âœ… 1. **Alta disponibilidad (High Availability)**
- La replicaciÃ³n permite que una base de datos estÃ© disponible incluso si el servidor principal falla. Los datos se copian a uno o mÃ¡s servidores de rÃ©plica, que pueden asumir el rol del servidor principal en caso de fallo.


### âœ… 2. **Balanceo de carga (Load Balancing)**
- Las rÃ©plicas pueden ser utilizadas para distribuir la carga de trabajo, permitiendo que las consultas de solo lectura se ejecuten en los servidores de rÃ©plica, aliviando la carga del servidor principal.

### âœ… 3. **RecuperaciÃ³n ante desastres (Disaster Recovery)**
- En caso de un desastre, las rÃ©plicas pueden ser utilizadas para restaurar rÃ¡pidamente los datos y minimizar el tiempo de inactividad.

### âœ… 4. **AnÃ¡lisis sin afectar producciÃ³n**
- Puedes hacer anÃ¡lisis pesados o pruebas en una rÃ©plica sin afectar el rendimiento del servidor principal.

### âœ… 5. **Migraciones o actualizaciones**
- Puedes usar una rÃ©plica lÃ³gica para migrar datos entre versiones diferentes de PostgreSQL o hacia otro sistema.

### âœ… 6. **IntegraciÃ³n con otros sistemas**
- La replicaciÃ³n lÃ³gica permite enviar cambios en tiempo real a sistemas como Kafka, Elasticsearch, BigQuery, etc.

---

## ğŸ”„ Â¿CuÃ¡ndo usar cada tipo?

| Objetivo | Tipo de rÃ©plica recomendada | Â¿Por quÃ©? |
|---------|------------------------------|-----------|
| Alta disponibilidad | **Streaming (fÃ­sica)** | RÃ©plica exacta del servidor, lista para tomar el control. |
| Balanceo de carga | **Streaming (fÃ­sica)** | Ideal para consultas de solo lectura. |
| AnÃ¡lisis o BI | **LÃ³gica** | Puedes replicar solo ciertas tablas. |
| IntegraciÃ³n con otros sistemas | **LÃ³gica** | Permite enviar cambios en formato JSON o eventos. |
| MigraciÃ³n entre versiones | **LÃ³gica** | Compatible entre versiones distintas. |

### ğŸ” ComparaciÃ³n: ReplicaciÃ³n en Streaming vs. ReplicaciÃ³n LÃ³gica

 

| **CaracterÃ­stica**             | **ReplicaciÃ³n en Streaming (FÃ­sica)**                              | **ReplicaciÃ³n LÃ³gica**                                                                 |
|-------------------------------|---------------------------------------------------------------------|----------------------------------------------------------------------------------------|
| **Nivel de replicaciÃ³n**      | A nivel de bloque de disco (fÃ­sico)                                | A nivel de cambios en filas y tablas (lÃ³gico)                                         |
| **Uso del WAL**               | Se transmite tal cual, binario                                     | Se decodifica el WAL en cambios lÃ³gicos (INSERT, UPDATE, DELETE)                     |
| **Requiere estructura idÃ©ntica** | SÃ­ (mismo esquema, extensiones, etc.)                              | No necesariamente (puede haber diferencias en tablas, columnas, etc.)                |
| **Flexibilidad**              | Limitada                                                           | Alta (puedes replicar solo algunas tablas, transformar datos, etc.)                  |
| **Casos de uso**              | Alta disponibilidad, failover                                      | IntegraciÃ³n, migraciones, replicaciÃ³n parcial, multi-master (con herramientas externas) |
| **TransmisiÃ³n del WAL**       | Se transmite el WAL completo en formato binario a cada rÃ©plica     | Cada suscriptor recibe solo los cambios relevantes del publicar, y ya decodificados desde el WAL usando el plugin lÃ³gico (pgoutput, wal2json, etc.)       |
 

---
 
### ğŸ”¹ **Failover**
El **failover** ocurre cuando el **nodo primario** falla inesperadamente y un nodo **standby** se convierte automÃ¡ticamente en el nuevo **nodo primario**.  
âœ… Se activa de manera automÃ¡tica en sistemas configurados con monitoreo y failover.  
âœ… Evita la caÃ­da total del servicio.  
âœ… Se usa en escenarios de emergencia cuando el servidor principal deja de funcionar.  
Ejemplo de comando manual:
 
### ğŸ”¹ **Switchover**
El **switchover** es un proceso planificado en el que el nodo **primario** y uno **standby** intercambian roles de forma controlada.  
âœ… Se realiza sin que haya fallos en el sistema.  
âœ… Se usa para mantenimiento o actualizaciones del nodo primario.  
âœ… Permite cambiar de lÃ­der sin interrupciones.  
 

ğŸ’¡ **Diferencia clave:**  
- **Failover** = Evento inesperado, ocurre por una falla.  
- **Switchover** = Cambio intencional y programado.


---

## âš ï¸ Consideraciones

- La **replicaciÃ³n fÃ­sica** es mÃ¡s simple y rÃ¡pida, pero menos flexible.
- La **replicaciÃ³n lÃ³gica** es mÃ¡s flexible (puedes elegir tablas), pero requiere mÃ¡s configuraciÃ³n.
- Ambas pueden coexistir si configuras `wal_level = logical`.

 

## ğŸ” Niveles de `wal_level` en PostgreSQL

### 1. `minimal`
- **Â¿QuÃ© hace?**  
  Registra solo lo estrictamente necesario para la recuperaciÃ³n ante fallos.
- **Uso tÃ­pico:**  
  Operaciones de carga masiva (`COPY`, `INSERT`) cuando no se necesita replicaciÃ³n ni puntos de recuperaciÃ³n avanzados.
- **Ventajas:**  
  - Menor volumen de WAL.
  - Mejor rendimiento en operaciones masivas.
- **Desventajas:**  
  - No permite replicaciÃ³n.
  - No se puede usar para backups PITR (Point-In-Time Recovery).

---

### 2. `replica` (antes llamado `archive`)
- **Â¿QuÃ© hace?**  
  Registra suficiente informaciÃ³n para replicaciÃ³n fÃ­sica y recuperaciÃ³n PITR.
- **Uso tÃ­pico:**  
  ReplicaciÃ³n fÃ­sica entre servidores PostgreSQL (standby).
- **Ventajas:**  
  - Permite replicaciÃ³n fÃ­sica.
  - Compatible con herramientas de backup como `pg_basebackup`.
- **Desventajas:**  
  - No permite replicaciÃ³n lÃ³gica.
  - MÃ¡s volumen de WAL que `minimal`.

---

### 3. `logical`
- **Â¿QuÃ© hace?**  
  Registra todos los cambios necesarios para replicaciÃ³n lÃ³gica (como `wal2json`, `pgoutput`, etc.).
- **Uso tÃ­pico:**  
  - ReplicaciÃ³n lÃ³gica.
  - IntegraciÃ³n con sistemas externos (Kafka, Debezium, etc.).
  - Captura de datos en cambio (CDC).
- **Ventajas:**  
  - Permite replicaciÃ³n lÃ³gica y fÃ­sica.
  - Ideal para arquitecturas orientadas a eventos.
- **Desventajas:**  
  - Mayor volumen de WAL.
  - Puede impactar el rendimiento si no se gestiona bien.

 

# Conceptos que se usan en las replicas 
```sql

Maestro/Primario
Esclavo/secundario/standby

- **Activo-Activo**:  En PostgreSQL 16, se ha mejorado la replicaciÃ³n lÃ³gica para permitir una configuraciÃ³n Activo-Activo, donde dos instancias de PostgreSQL pueden recibir escrituras simultÃ¡neamente y sincronizar los cambios entre ellas.
En este modelo, todos los nodos estÃ¡n operativos y procesan solicitudes y cambios simultÃ¡neamente. Esto permite distribuir la carga de trabajo entre mÃºltiples servidores, mejorando el rendimiento y la disponibilidad. Si un nodo falla, los demÃ¡s continÃºan funcionando sin interrupciones.

- **Activo-Pasivo**: AquÃ­, solo un nodo estÃ¡ activo y maneja las solicitudes, mientras que otro nodo permanece en espera (pasivo). Si el nodo activo falla, el pasivo puede toma el control mediante failover. Este enfoque es mÃ¡s simple y garantiza estabilidad, pero no aprovecha los recursos del nodo pasivo hasta que sea necesario.

El **split-brain** es un problema que ocurre en sistemas de alta disponibilidad y replicaciÃ³n, cuando dos nodos **pierden comunicaciÃ³n entre sÃ­**, pero **ambos creen que son el primario** al mismo tiempo.
  
----------------------------------------------------------------------------------------------------------------------------------------------------------------

Â¿QuÃ© significa el consenso en tÃ©rminos generales?  Es un acuerdo entre mÃºltiples participantes â†’ Un grupo debe tomar una decisiÃ³n colectiva basada en reglas claras.  Evita que haya decisiones individuales incorrectas â†’ Por ejemplo, en replicaciÃ³n de bases de datos, un nodo no puede decidir solo convertirse en primario sin confirmaciÃ³n de los demÃ¡s.  Se usa en algoritmos de failover y gestiÃ³n de sistemas distribuidos â†’ Como Raft, Paxos y Etcd, que permiten que los servidores acuerden quiÃ©n es el lÃ­der.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

CAP Theorem â†’ En bases de datos distribuidas, puedes tener Consistencia (C), Disponibilidad (A) o Tolerancia a Particiones (P), pero nunca las tres simultÃ¡neamente.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

Un Data Warehouse es un sistema de almacenamiento y gestiÃ³n de datos diseÃ±ado para facilitar el anÃ¡lisis y la toma de decisiones en una organizaciÃ³n. Funciona como un repositorio central donde se integran y estructuran grandes volÃºmenes de informaciÃ³n provenientes de mÃºltiples fuentes.

```

## **Raft** y **Paxos** 
 son algoritmos de *consenso distribuido*, diseÃ±ados para que mÃºltiples nodos en un sistema lleguen a un acuerdo sobre un valor, incluso si algunos fallan o se desconectan. Son fundamentales en bases de datos distribuidas, sistemas de archivos y clÃºsteres de alta disponibilidad.


###  **Â¿QuÃ© es Paxos?**

- Propuesto por Leslie Lamport en los aÃ±os 80.
- Es un algoritmo robusto pero **difÃ­cil de entender e implementar**.
- Usa tres roles:
  - **Proposers**: proponen valores.
  - **Acceptors**: aceptan o rechazan propuestas.
  - **Learners**: aprenden el valor acordado.
- Funciona por medio de rondas de propuestas y promesas, buscando que **una mayorÃ­a (quÃ³rum)** acepte un valor.

 *Ventaja:* muy tolerante a fallos.  
 *Desventaja:* complejo, propenso a errores de implementaciÃ³n.


###ï¸ **Â¿QuÃ© es Raft?**

- DiseÃ±ado en 2013 por Diego Ongaro y John Ousterhout como una alternativa mÃ¡s **entendible** a Paxos.
- Usa un enfoque **basado en liderazgo**:
  - Un nodo es elegido como **lÃ­der**.
  - Los demÃ¡s son **seguidores**.
  - Si el lÃ­der falla, se realiza una **elecciÃ³n** para elegir uno nuevo.
- El lÃ­der recibe las operaciones y las **replica en todos los nodos**.

 *Ventaja:* mÃ¡s fÃ¡cil de implementar y razonar.  
*Desventaja:* no es tolerante a fallos bizantinos (no protege contra nodos maliciosos).


---

### ğŸ”‘ Â¿QuÃ© es el quÃ³rum?

Es **la mayorÃ­a necesaria de nodos de consenso (por ejemplo, etcd)** que deben estar **activos y en acuerdo** para que se puedan tomar decisiones crÃ­ticas de manera segura.  
Ejemplo clÃ¡sico: en un clÃºster de 3 nodos etcd, **se necesita al menos 2 funcionando** para tener quÃ³rum. Herramientas como etcd exige quorum y si no hay mayorÃ­a (quorum) de nodos disponibles no aceptarÃ¡ escrituras ni permitirÃ¡ elecciones de lÃ­der 

### ğŸ“Œ Reglas clave:

- El **quÃ³rum se calcula sobre los nodos consenso como etcd**, **no sobre los servidores PostgreSQL**.
- Siempre necesitas al menos **una mayorÃ­a de nodos etcd funcionales** para que Patroni pueda tomar decisiones crÃ­ticas como un failover.
- **Debe ser siempre un nÃºmero impar** para facilitar la mayorÃ­a.
- FÃ³rmula: Para tolerar _f_ fallos â†’ necesitas **2Ã—f + 1** nodos etcd.


### ğŸ§  Â¿Por quÃ© es tan importante?

Porque sin quÃ³rum:

- No se puede promover una rÃ©plica a primario.
- El sistema entra en estado de seguridad (failover bloqueado).
- Se evita el *split-brain* (dos nodos creyÃ©ndose lÃ­deres al mismo tiempo).


### ConfiguraciÃ³n de nodos etcd y tolerancia a fallos

| # Nodos etcd | QuÃ³rum necesario | Fallos tolerables | Â¿CuÃ¡ndo usarlo?                              |
|--------------|------------------|-------------------|----------------------------------------------|
| 1 (no recomendado) | 1                | 0                 | Solo para pruebas locales â€“ âŒ Punto Ãºnico de fallo |
| 3 (ideal mÃ­nimo)   | 2                | 1                 | ProducciÃ³n bÃ¡sica                            |
| 5                 | 3                | 2                 | Alta disponibilidad en mÃºltiples zonas       |
| 7                 | 4                | 3                 | Infraestructura crÃ­tica o multinube          |
 
  
### ğŸ§  **Â¿CuÃ¡ndo deberÃ­as considerar aumentar los nodos de consenso?**
Saber cuÃ¡ndo aumentar el nÃºmero de nodos en tu clÃºster de consenso (como etcd) no depende de cuÃ¡ntos servidores PostgreSQL tengas, sino de cuÃ¡nto Fallos de consenso estÃ¡s dispuesto a tolerar y quÃ© tan crÃ­tica es tu infraestructura y en un sistema donde se prestan fallos comunes.

1. **Cuando necesitas tolerar mÃ¡s fallos**
   - Si actualmente tienes 3 nodos etcd, solo puedes tolerar 1 caÃ­da.
   - Si quieres tolerar 2 fallos simultÃ¡neos, necesitas 5 nodos.

 ---

# Conocimiento esencial para diseÃ±ar arquitecturas distribuidas eficientes.  

- Ley de Amdahl
- Ley de Gunther
- FÃ³rmula de latencia en redes distribuidas
- FÃ³rmula de Throughput
- FÃ³rmula de Consistencia CAP
- Teorema de Brewer (PACELC)
- Ley de Little
- FÃ³rmula de escalabilidad de Gustafson
---

### **ğŸ“Œ Ley de Amdahl â€“ LÃ­mite de aceleraciÃ³n en paralelizaciÃ³n**  

ğŸ“ **FÃ³rmula general**  

$$  S = \frac{1}{(1 - P) + \frac{P}{N}}  $$

ğŸ“ **Significado de cada variable**  
- **S (Speedup)** â†’ **Variable**: Es el resultado final de cuÃ¡nto mejora el rendimiento del sistema.  
- **P (Parallelizable Fraction)** â†’ **Variable**: Porcentaje del sistema que puede ejecutarse en paralelo.  
- **N (Number of Processors)** â†’ **Variable**: Cantidad de nodos o procesadores usados en paralelo.  
- **(1 - P)** â†’ **Constante**: Representa la parte del sistema que **siempre serÃ¡ secuencial** y no se puede paralelizar.  

ğŸ“ **Ejemplo prÃ¡ctico**  
Supongamos que queremos procesar un conjunto de datos en PostgreSQL:  
- **El 80% de la tarea puede ejecutarse en paralelo** (`P = 0.8`).  
- **Usaremos 4 servidores** (`N = 4`).  

Aplicamos la fÃ³rmula:  

$$ S = \frac{1}{(1 - 0.8) + \frac{0.8}{4}} $$  

$$ S = \frac{1}{0.2 + 0.2} $$

$$ S = \frac{1}{0.4} = 2.5 $$

ğŸ“Œ **ConclusiÃ³n**  
Aunque agreguemos **4 nodos**, el sistema solo se vuelve **2.5 veces mÃ¡s rÃ¡pido**, porque aÃºn hay una fracciÃ³n **(1 - P)** que nunca podrÃ¡ paralelizarse. Este principio es clave en sistemas distribuidos: mÃ¡s servidores no siempre significan mÃ¡s velocidad.

---

### **ğŸ“Œ Ley de Gunther â€“ LÃ­mite de escalabilidad en un sistema**  

ğŸ“ **FÃ³rmula general**  

$$ X(N) = \frac{N}{1 + \sigma (N - 1)} $$

ğŸ“ **Significado de cada variable**  
- **X(N) (Rendimiento escalado)** â†’ **Variable**: Resultado final de cuÃ¡nto mejora el rendimiento real del sistema con `N` nodos.  
- **N (Number of Nodes)** â†’ **Variable**: Cantidad de servidores en el sistema distribuido.  
- **Ïƒ (Contention Factor)** â†’ **Variable**: Porcentaje de contenciÃ³n por recursos compartidos en el sistema.  
- **El nÃºmero "1" en el denominador** â†’ **Constante**: Representa la ejecuciÃ³n sin contenciÃ³n.  

ğŸ“ **Ejemplo prÃ¡ctico**  
Supongamos que queremos **ampliar un clÃºster de bases de datos** con Citus:  
- **Tenemos 10 nodos** (`N = 10`).  
- **La contenciÃ³n causada por comunicaciÃ³n es 30%** (`Ïƒ = 0.3`).  

Aplicamos la fÃ³rmula:  

$$ X(10) = \frac{10}{1 + 0.3 (10 - 1)} $$

$$ X(10) = \frac{10}{1 + 2.7} $$

$$ X(10) = \frac{10}{3.7} = 2.7 $$

ğŸ“Œ **ConclusiÃ³n**  
Aunque agregamos **10 nodos**, el rendimiento **solo se multiplica por 2.7** debido a la contenciÃ³n de recursos compartidos. Esto demuestra que simplemente agregar mÃ¡s servidores no siempre es la mejor estrategia sin optimizaciÃ³n.

---

### **ğŸ“Œ FÃ³rmula de latencia en redes distribuidas**  

ğŸ“ **FÃ³rmula general**  

$$ L = RTT + \frac{S}{B} $$

ğŸ“ **Significado de cada variable**  
- **L (Latency)** â†’ **Variable**: Tiempo total que tarda una operaciÃ³n en completarse en el sistema distribuido.  
- **RTT (Round Trip Time)** â†’ **Constante**: Tiempo de ida y vuelta de los paquetes en la red.  
- **S (Size of Message)** â†’ **Variable**: TamaÃ±o del dato que se transmite en la red.  
- **B (Bandwidth)** â†’ **Variable**: Velocidad de transmisiÃ³n de la red (Mbps).  

ğŸ“ **Ejemplo prÃ¡ctico**  
Si tenemos una conexiÃ³n donde:  
- **RTT es 50 ms** (`RTT = 50`).  
- **El mensaje tiene 5 MB** (`S = 5000 KB`).  
- **El ancho de banda es 100 Mbps** (`B = 100000 KB/s`).  

Aplicamos la fÃ³rmula:  

$$ L = 50 + \frac{5000}{100000} $$

$$ L = 50 + 0.05 = 50.05 ms $$

ğŸ“Œ **ConclusiÃ³n**  
La latencia total es **50.05 ms**, y lo que mÃ¡s afecta el rendimiento es el **RTT**, que es una constante del sistema. Aunque se aumente el ancho de banda, el tiempo mÃ­nimo de ida y vuelta **siempre serÃ¡ 50 ms**.


 
---
 

### **ğŸ“Œ FÃ³rmula de Throughput â€“ Capacidad del sistema para procesar operaciones**  

ğŸ“ **FÃ³rmula general**  

$$ T = \frac{N}{L} $$  

ğŸ“ **Significado de cada variable**  
- **T (Throughput)** â†’ **Variable**: Indica cuÃ¡ntas operaciones por segundo puede manejar el sistema.  
- **N (Number of Transactions)** â†’ **Variable**: Es la cantidad total de operaciones que el sistema debe procesar.  
- **L (Latency per Transaction)** â†’ **Variable**: Tiempo que toma cada operaciÃ³n en completarse.  

ğŸ“ **Valores constantes:**  
âœ… **La estructura de la ecuaciÃ³n** â†’ Siempre serÃ¡ una **divisiÃ³n entre cantidad de operaciones y su latencia**, ya que el concepto de rendimiento no cambia.  

ğŸ“ **Ejemplo prÃ¡ctico**  
Imaginemos que tenemos un sistema distribuido con **10,000 operaciones** (`N = 10,000`) y cada transacciÃ³n tarda **500 ms** (`L = 0.5 segundos`). Aplicamos la fÃ³rmula:  

$$ T = \frac{10,000}{0.5} $$  

$$ T = 20,000 \text{ operaciones/segundo} $$  

ğŸ“Œ **ConclusiÃ³n**  
Este sistema es capaz de procesar **20,000 operaciones por segundo**. Si queremos mejorar el rendimiento, podemos:  
- **Reducir la latencia (`L`)** optimizando consultas.  
- **Aumentar la cantidad de nodos** para procesar mÃ¡s transacciones en paralelo.  


---
### **ğŸ“Œ FÃ³rmula de Consistencia CAP â€“ Equilibrio en sistemas distribuidos**  

ğŸ“ **FÃ³rmula general**  
El **Teorema CAP** establece que un sistema distribuido **puede garantizar solo dos de tres propiedades**:  

$$  C + A + P \neq 3 $$  

Donde:  
- **C (Consistency)** â†’ **Variable**: Garantiza que todos los nodos ven los mismos datos al mismo tiempo.  
- **A (Availability)** â†’ **Variable**: Asegura que cada solicitud recibe una respuesta, incluso si algunos nodos fallan.  
- **P (Partition Tolerance)** â†’ **Constante**: El sistema sigue funcionando a pesar de fallos en la red.  

ğŸ“ **Ejemplo prÃ¡ctico**  
Supongamos que tenemos una base de datos distribuida y ocurre una **falla de red**.  
- Si priorizamos **Consistencia (C) y ParticiÃ³n (P)**, el sistema **rechazarÃ¡ algunas solicitudes** para garantizar datos correctos.  
- Si priorizamos **Disponibilidad (A) y ParticiÃ³n (P)**, el sistema **seguirÃ¡ respondiendo**, pero algunos datos pueden estar desactualizados.  

ğŸ“Œ **ConclusiÃ³n**  
No es posible tener **las tres propiedades al mismo tiempo**. Cada sistema debe elegir entre **CP (consistencia y tolerancia a fallos)** o **AP (disponibilidad y tolerancia a fallos)** segÃºn sus necesidades.  

---
### **ğŸ“Œ Teorema de Brewer (PACELC) â€“ ExtensiÃ³n del CAP Theorem**  
El **PACELC Theorem** es una extensiÃ³n del **CAP Theorem**, que introduce un nuevo concepto clave en sistemas distribuidos: **latencia**.  

ğŸ“Œ **Â¿QuÃ© significa PACELC?**  
El acrÃ³nimo representa:  
- **P** â†’ **Partitioning** (ParticiÃ³n en la red).  
- **A** â†’ **Availability** (Disponibilidad).  
- **C** â†’ **Consistency** (Consistencia).  
- **E** â†’ **Else** (Si no hay particiÃ³n).  
- **L** â†’ **Latency** (Latencia).  
- **C** â†’ **Consistency** (Consistencia).  

ğŸ“ **FÃ³rmula conceptual**  
Si hay **particiÃ³n en la red**, el sistema debe elegir entre **Consistencia (C) o Disponibilidad (A)**.  
Si **no hay particiÃ³n**, el sistema debe elegir entre **Latencia baja (L) o Consistencia (C)**.  

ğŸ“Œ **Importancia:**  
Este teorema amplÃ­a el **CAP Theorem**, agregando la dimensiÃ³n de **latencia** en sistemas distribuidos.  

ğŸ“Œ **Â¿CÃ³mo funciona PACELC?**  
El teorema establece dos escenarios:  
1. **Si hay una particiÃ³n en la red (P)** â†’ Se debe elegir entre **Disponibilidad (A) o Consistencia (C)** (como en el CAP Theorem).  
2. **Si no hay particiÃ³n (Else - E)** â†’ Se debe elegir entre **Latencia baja (L) o Consistencia (C)**.  

ğŸ“Œ **Ejemplo prÃ¡ctico**  
- **Google Spanner** prioriza **consistencia** en todo momento (**PC/EC**).  
- **Cassandra** prioriza **disponibilidad y baja latencia** (**PA/EL**).  

ğŸ“Œ **Importancia**  
PACELC mejora el CAP Theorem al considerar **rendimiento y latencia**, lo que es crucial en bases de datos distribuidas y aplicaciones en la nube.  

ğŸ“Œ **DÃ³nde se usa:**  
- DiseÃ±o de **bases de datos distribuidas** como Cassandra, Spanner y Citus.  
- EvaluaciÃ³n de **arquitecturas de microservicios**.  
- OptimizaciÃ³n de **sistemas de almacenamiento en la nube**.  

--- 
 
### **ğŸ“Œ Ley de Little â€“ RelaciÃ³n entre tiempo de respuesta y concurrencia**  
ğŸ“ **FÃ³rmula general**  

$$ L = \lambda W $$
 
ğŸ“ **Significado de cada variable**  
- **L (Longitud de la cola)** â†’ **Variable**: NÃºmero promedio de solicitudes en espera en el sistema.  
- **Î» (Tasa de llegada)** â†’ **Variable**: Cantidad de solicitudes que llegan por unidad de tiempo.  
- **W (Tiempo de espera promedio)** â†’ **Variable**: Tiempo que cada solicitud pasa en el sistema.  

ğŸ“Œ **Importancia:**  
Ayuda a calcular **cuÃ¡nto trÃ¡fico puede manejar un sistema distribuido** antes de que se vuelva lento.  

ğŸ“Œ **DÃ³nde se usa:**  
- DiseÃ±o de **balanceo de carga** en servidores.  
- OptimizaciÃ³n de **colas de procesamiento** en bases de datos.  
- EvaluaciÃ³n de **rendimiento en APIs** y sistemas web.  

---

 
### **ğŸ“Œ FÃ³rmula de escalabilidad de Gustafson â€“ CorrecciÃ³n de la Ley de Amdahl**  
ğŸ“ **FÃ³rmula general**  

$$  S = N - (1 - P) (N - 1) $$  

ğŸ“ **Significado de cada variable**  
- **S (Speedup)** â†’ **Variable**: AceleraciÃ³n del sistema con paralelizaciÃ³n.  
- **N (Number of Processors)** â†’ **Variable**: NÃºmero de nodos o procesadores usados.  
- **P (Parallelizable Fraction)** â†’ **Variable**: Porcentaje del sistema que puede ejecutarse en paralelo.  

ğŸ“Œ **Importancia:**  
Corrige la **Ley de Amdahl**, mostrando que **mÃ¡s nodos pueden mejorar el rendimiento** si el problema se escala correctamente.  

ğŸ“Œ **DÃ³nde se usa:**  
- DiseÃ±o de **clusters de computaciÃ³n distribuida**.  
- OptimizaciÃ³n de **procesamiento en paralelo** en bases de datos.  
- EvaluaciÃ³n de **rendimiento en sistemas de Big Data**.  


---


 

## BibliografÃ­a 
```
https://www.youtube.com/watch?v=kW8xT_cgEMM
https://medium.com/@c.ucanefe/patroni-ha-proxy-feed1292d23f


https://www.geeksforgeeks.org/paxos-vs-raft-algorithm-in-distributed-systems/
https://dev.to/pragyasapkota/consensus-algorithms-paxos-and-raft-37ab

```
