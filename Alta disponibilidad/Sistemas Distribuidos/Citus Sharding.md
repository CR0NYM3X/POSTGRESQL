**Citus** es una extensiÃ³n de PostgreSQL que permite **escalar bases de datos distribuidas**, ideal para manejar grandes volÃºmenes de datos y mejorar el rendimiento en sistemas con mÃºltiples nodos. Citus usa logical decoding para mover datos entre nodos.  esta herramienta se recomienda usar en sistemas donde requieran realizar analisis de una gran cantidad de datos. convierte a postgresql OLTP (Procesamiento de Transacciones en LÃ­nea) en un OLAP (Procesamiento AnalÃ­tico en LÃ­nea)

### **Â¿Por quÃ© esto es potente?**
1. **Escalabilidad horizontal** â†’ En lugar de que un solo servidor maneje toda la carga, los datos se distribuyen en mÃºltiples workers, mejorando el rendimiento.
2. **Procesamiento paralelo** â†’ Las consultas se ejecutan en todos los workers simultÃ¡neamente, reduciendo los tiempos de respuesta.
3. **Carga distribuida** â†’ Si tienes millones de registros, cada worker maneja solo una parte, evitando que el coordinador se convierta en un cuello de botella.
4. **Alta disponibilidad** â†’ Si un worker falla, los shards pueden reasignarse, evitando pÃ©rdida de datos y permitiendo recuperaciÃ³n rÃ¡pida.


 ### **Conceptos clave de Citus**
  
1. **Coordinador** â†’ Es el nodo principal que recibe las consultas y  distribuye la consulta a los **workers nodes**. El coordinador no almacena los datos, solo dirige las consultas hacia los workers. el coordinador es donde se administra la distribucion y se consulta la informaciÃ³n, por lo tanto ahÃ­ solo se debe hacer los UPDATE,DELETE , INSERT.
2. **Workers** â†’ Almacenan fragmentos shard de la tabla  y procesan las consultas asignadas por el coordinador, asi dividiendo el trabajo en varios nodos. tambiÃ©n se puede consultar las tablas desde los worker
3. **sharding** â†’ Es el proceso de dividir una tabla grande en muchas tablas mÃ¡s pequeÃ±as (fragmentos o shards) y repartirlas entre los nodos trabajadores. Citus automÃ¡ticamente asigna y gestiona estos shards en cada worker node, pero no los trata como tablas convencionales en el catÃ¡logo de PostgreSQL (pg_class).

#### **ðŸ”¹ Â¿CÃ³mo se distribuyen los datos en Citus?**
Cuando creas una tabla distribuida con `create_distributed_table()`, Citus **divide los registros en shards** y los asigna a distintos workers nodes. AsÃ­, en lugar de almacenar toda la tabla en un solo servidor, los datos se distribuyen entre los nodos.

  **Ejemplo de flujo en Citus**
- **InserciÃ³n de datos** â†’ El Coordinador decide en quÃ© Worker debe ir cada fila.
- **Consultas** â†’ El Coordinador consulta a los Workers y combina los resultados.
- **Paralelismo** â†’ Cada Worker procesa parte de la carga, mejorando el rendimiento.

 
## **QuÃ© pasa si un **worker node** se pierde**
los datos que estaban en ese nodo pueden volverse inaccesibles**, afectando las consultas distribuidas. 

### **ðŸ”¹ Impacto de perder un worker**
ðŸ“Œ **Si un worker falla, ocurre lo siguiente:**  
1. **Las consultas que dependen de los shards en ese nodo pueden fallar**, mostrando errores de conexiÃ³n.  
2. **El coordinador seguirÃ¡ funcionando**, pero no podrÃ¡ recuperar datos almacenados en el nodo caÃ­do.  
3. **Si la tabla distribuida es replicada, otro nodo puede asumir la carga y evitar pÃ©rdida de datos.**  

#  Que puedes hacer con Citus

## âœ… **1. DistribuciÃ³n de Tablas**

*   **create\_distributed\_table()**: Convierte una tabla en distribuida, dividiÃ©ndola en *shards* que se reparten entre nodos.
*   MÃ©todos de distribuciÃ³n:
    *   **Hash**: Ideal para multi-tenant (por `tenant_id`).
    *   **Append**: Para datos que crecen en el tiempo (eventos, logs).

## âœ… **2. Tablas de Referencia**

*   **create\_reference\_table()**: Replica tablas pequeÃ±as en todos los nodos para joins rÃ¡pidos (ej. catÃ¡logos, paÃ­ses).

## âœ… **3. Particionamiento Temporal**

*   **create\_time\_partitions()**: Crea particiones por rango de tiempo sobre tablas distribuidas.
*   Permite **retenciÃ³n automÃ¡tica** (drop partitions) y optimizaciÃ³n de consultas por fecha.

## âœ… **4. Consultas Distribuidas**

*   Ejecuta **consultas paralelas** en todos los nodos.
*   Compatible con:
    *   **JOINs** entre tablas distribuidas y de referencia.
    *   **Aggregates** (SUM, COUNT, AVG) distribuidos.
    *   **Subconsultas** y **CTEs** (con ciertas limitaciones).

## âœ… **5. Escalabilidad Horizontal**

*   AÃ±adir nodos para aumentar capacidad.
*   Redistribuir shards con **rebalanceo automÃ¡tico**.

## âœ… **6. Alta Disponibilidad**

*   IntegraciÃ³n con **replicaciÃ³n** (streaming replication).
*   Failover y resiliencia ante caÃ­das.

## âœ… **7. IntegraciÃ³n con PostgreSQL**

*   Compatible con **Ã­ndices**, **constraints**, **triggers** (con restricciones).
*   Soporte para **Postgres 16+** y caracterÃ­sticas modernas.

## âœ… **8. Herramientas Avanzadas**

*   **pg\_cron** + Citus: AutomatizaciÃ³n de tareas (crear/dropear particiones).
*   **Citus MX**: Para cargas mixtas OLTP + OLAP.
*   **Columnar Storage**: Para analÃ­tica (almacenamiento por columnas).

## âœ… **9. Casos de Uso**

*   **SaaS multi-tenant**: Distribuir por `tenant_id`.
*   **AnalÃ­tica en tiempo real**: Distribuir por tiempo y agregar datos masivos.
*   **IoT / Logs**: Particiones por fecha + distribuciÃ³n por dispositivo.


 
---

### **ðŸ–¥ï¸ Escenario del laboratorio**
Se crearon **cuatro servidores** en una red privada que formarÃ¡n el clÃºster, Este esquema permitirÃ¡ repartir la carga de trabajo y escalar el sistema de manera eficiente.
se utilizara citus columnar para mejorar  la compresiÃ³n y acelerar las consultas, reduciendo el uso de disco y mejorando el rendimiento en agregaciones como avg(), sum() etc... insertaremos mil millones de registros. si no usaramos citus columar se necesitarian 350GB en disco. utilizaremos unicamente las configuraciones por default de postgresql y nuestro servidor tiene 24 GB de Ram, 15 nucleos y S.O Red Hat .

- **Coordinador**: 127.0.0.1 Puerto 55164 *(Maneja las consultas y distribuye los datos)*
- **Worker 1**: 127.0.0.1 Puerto 55165 *(Almacena parte de los datos y ejecuta queries)*
- **Worker 2**: 127.0.0.1 Puerto 55166 *(Almacena parte de los datos y ejecuta queries)*
- **Worker 3**: 127.0.0.1 Puerto 55167 *(Nuevo nodo integrado ya despues de la implementaciÃ³n)*

---
 ![Logo de GitHub](https://github.com/CR0NYM3X/POSTGRESQL/blob/main/Alta%20disponibilidad/Sistemas%20Distribuidos/img/diagrama.jpeg)



### **Crear las carpetas donde se guarda el data de postgresql**
```bash
mkdir -p  /sysx/data16/DATANEW/data_coordinador
mkdir -p  /sysx/data16/DATANEW/data_worker1
mkdir -p  /sysx/data16/DATANEW/data_worker2
mkdir -p  /sysx/data16/DATANEW/data_worker3
```

### **Iniciarlizar los data**
```bash
/usr/pgsql-16/bin/initdb -E UTF-8 -D  /sysx/data16/DATANEW/data_coordinador --data-checksums  &>/dev/null
/usr/pgsql-16/bin/initdb -E UTF-8 -D  /sysx/data16/DATANEW/data_worker1 --data-checksums  &>/dev/null
/usr/pgsql-16/bin/initdb -E UTF-8 -D  /sysx/data16/DATANEW/data_worker2 --data-checksums  &>/dev/null
/usr/pgsql-16/bin/initdb -E UTF-8 -D  /sysx/data16/DATANEW/data_worker3 --data-checksums  &>/dev/null
```

### **configurar el postgresql.auto.conf de cada nodo**
```bash
# Habilita la nivel de wal logico
echo "wal_level = logical" >> /sysx/data16/DATANEW/data_coordinador/postgresql.auto.conf
echo "wal_level = logical" >> /sysx/data16/DATANEW/data_worker1/postgresql.auto.conf
echo "wal_level = logical" >> /sysx/data16/DATANEW/data_worker2/postgresql.auto.conf
echo "wal_level = logical" >> /sysx/data16/DATANEW/data_worker3/postgresql.auto.conf

# Cargar la liberia citus en todos los nodos 
echo "shared_preload_libraries = 'citus'" >> /sysx/data16/DATANEW/data_coordinador/postgresql.auto.conf
echo "shared_preload_libraries = 'citus'" >> /sysx/data16/DATANEW/data_worker1/postgresql.auto.conf
echo "shared_preload_libraries = 'citus'" >> /sysx/data16/DATANEW/data_worker2/postgresql.auto.conf
echo "shared_preload_libraries = 'citus'" >> /sysx/data16/DATANEW/data_worker3/postgresql.auto.conf

# Cambiar los puertos en cada nodo
echo "port = 55164" >> /sysx/data16/DATANEW/data_coordinador/postgresql.auto.conf
echo "port = 55165" >> /sysx/data16/DATANEW/data_worker1/postgresql.auto.conf
echo "port = 55166" >> /sysx/data16/DATANEW/data_worker2/postgresql.auto.conf
echo "port = 55166" >> /sysx/data16/DATANEW/data_worker3/postgresql.auto.conf

# Permitir las conexiones externas 
echo "listen_addresses = '*'" >> /sysx/data16/DATANEW/data_coordinador/postgresql.auto.conf
echo "listen_addresses = '*'" >> /sysx/data16/DATANEW/data_worker1/postgresql.auto.conf
echo "listen_addresses = '*'" >> /sysx/data16/DATANEW/data_worker2/postgresql.auto.conf
echo "listen_addresses = '*'" >> /sysx/data16/DATANEW/data_worker3/postgresql.auto.conf
```

### **configurar el pg_hba.conf de cada nodo**
[NOTA] -> Esto es opcional solo si en tu laboratorio ya tienes los servidores , en mi caso como yo lo estoy haciendo en local, no ocupo configurarlo ya que por defaul cuando inicializas el data ya tiene permiso trust en local
```
host    all    all    127.0.01/32    trust
```

### **Iniciar el servicio de todos los nodos**
```
/usr/pgsql-16/bin/pg_ctl start -D /sysx/data16/DATANEW/data_coordinador
/usr/pgsql-16/bin/pg_ctl start -D /sysx/data16/DATANEW/data_worker1
/usr/pgsql-16/bin/pg_ctl start -D /sysx/data16/DATANEW/data_worker2
/usr/pgsql-16/bin/pg_ctl start -D /sysx/data16/DATANEW/data_worker3
```

### **Instalar la extension en todos los nodos**
```
psql -p 55164 -c "CREATE EXTENSION citus;" -- este ya instala la extension de citus_columnar
psql -p 55165 -c "CREATE EXTENSION citus;"
psql -p 55166 -c "CREATE EXTENSION citus;"
psql -p 55167 -c "CREATE EXTENSION citus;"
```

### Configurar el servidor  coordinador puerto 55164
```sql
--# Indica quien es el nodo coordinador
SELECT citus_set_coordinator_host('127.0.0.1', 55164);

--#  Indicar quien son los nodos workers 
SELECT * FROM citus_add_node('127.0.0.1', 55165);
SELECT * FROM citus_add_node('127.0.0.1', 55166);

-- #  Ver los nodos workers 
SELECT * FROM citus_get_active_worker_nodes();
```

### Crear tabla columnar y insertar registros en el coordinador
Las tablas columnares Son mÃ¡s de anÃ¡lisis y funcionan mejor con consultas agregadas y escaneo masivo.
Las consultas agregadas son aquellas que utilizan funciones como SUM(), AVG(), COUNT(), MIN(), MAX(), entre otras, para resumir datos.
```sql
CREATE TABLE users (
    id BIGINT PRIMARY KEY,
    username TEXT NOT NULL,
    email TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT now(),
    is_active BOOLEAN DEFAULT true
)  USING columnar;
```

### Empezar a distribuir la tabla
```sql
SELECT create_distributed_table('users', 'id');
```

### Crear index para la tabla 
```sql
-- Ãndice para bÃºsquedas por nombre de usuario
CREATE INDEX idx_users_username ON users(username,email);

-- Ãndice para bÃºsquedas por correo electrÃ³nico
CREATE INDEX idx_users_email ON users(email);

-- Ãndice para consultas por fecha de creaciÃ³n (Ãºtil para rangos de tiempo)
CREATE INDEX idx_users_created_at ON users(created_at);
```


### Visualizar 
```sql
postgres@postgres# \dt+
                                    List of relations
+--------+-------+-------+----------+-------------+---------------+-------+-------------+
| Schema | Name  | Type  |  Owner   | Persistence | Access method | Size  | Description |
+--------+-------+-------+----------+-------------+---------------+-------+-------------+
| public | users | table | postgres | permanent   | columnar      | 16 kB | NULL        |
+--------+-------+-------+----------+-------------+---------------+-------+-------------+
(1 row)

postgres@postgres# \d users
                            Table "public.users"
+------------+-----------------------------+-----------+----------+---------+
|   Column   |            Type             | Collation | Nullable | Default |
+------------+-----------------------------+-----------+----------+---------+
| id         | bigint                      |           | not null |         |
| username   | text                        |           | not null |         |
| email      | text                        |           | not null |         |
| created_at | timestamp without time zone |           |          | now()   |
| is_active  | boolean                     |           |          | true    |
+------------+-----------------------------+-----------+----------+---------+
Indexes:
    "users_pkey" PRIMARY KEY, btree (id)
    "idx_users_created_at" btree (created_at)
    "idx_users_email" btree (email)
    "idx_users_username" btree (username, email)

```


### **ðŸ”¹ Paso 1: Agregar el nuevo nodo**
 
ConÃ©ctate al **coordinador** y registra el nuevo worker:
```sql
SELECT * FROM citus_add_node('192.168.1.103', 5432);
```

### **ðŸ”¹ Paso 2: Verificar nodos activos**
Antes de redistribuir los datos, asegÃºrate de que el nuevo nodo estÃ¡ activo:
```sql
SELECT * FROM citus_get_active_worker_nodes();
```

### **ðŸ”¹ Paso 3: Redistribuir shards**
Ahora que el nuevo nodo estÃ¡ disponible, redistribuye los datos entre todos los workers:
```sql
SELECT rebalance_table_shards('users');

-- select * from citus_rebalance_status();

```

### **ðŸ”¹ Paso 4: Verifica la nueva redistribuciÃ³n con:**
```sql
SELECT * FROM citus_shards;
```

---


###   Crear tabla `puestos` (como referencia)

```sql
CREATE TABLE puestos (
    id SMALLINT PRIMARY KEY,
    nombre TEXT NOT NULL
);

-- Replica tablas pequeÃ±as en todos los nodos para joins rÃ¡pidos (ej. catÃ¡logos, paÃ­ses).
SELECT create_reference_table('puestos');
```

 
###  Insertar datos de ejemplo

```sql
INSERT INTO puestos (id, nombre) VALUES
(1, 'Director General'),
(2, 'Gerente de Ãrea'),
(3, 'Jefe de Departamento'),
(4, 'Analista'),
(5, 'Desarrollador'),
(6, 'Soporte TÃ©cnico'),
(7, 'Practicante');
```

---

###   En el servidor coordinador agregar nueva columna
Puedes ahora agregar una columna `puesto_id` a la tabla `users`:
```sql
ALTER TABLE users ADD COLUMN puesto_id SMALLINT;
```

 

---


### Insertar mil millones de registros
Esta operacion tarda tiempo
```bash
psql -p 55164 -c "INSERT INTO users (id, username, email, created_at, is_active)
SELECT i, 'user_' || i, 'user_' || i || '@example.com', now() - (i % 10000) * interval '1 second', (i % 2 = 0)
FROM generate_series(1, 250000000) AS i;" &

psql -p 55164  -c "INSERT INTO users (id, username, email, created_at, is_active)
SELECT i, 'user_' || i, 'user_' || i || '@example.com', now() - (i % 10000) * interval '1 second', (i % 2 = 0)
FROM generate_series(250000001, 500000000) AS i;" &

psql -p 55164  -c "INSERT INTO users (id, username, email, created_at, is_active)
SELECT i, 'user_' || i, 'user_' || i || '@example.com', now() - (i % 10000) * interval '1 second', (i % 2 = 0)
FROM generate_series(500000001, 750000000) AS i;" &

psql -p 55164  -c "INSERT INTO users (id, username, email, created_at, is_active)
SELECT i, 'user_' || i, 'user_' || i || '@example.com', now() - (i % 10000) * interval '1 second', (i % 2 = 0)
FROM generate_series(750000001, 1000000000) AS i;" &


ps -fea | grep generate_series

```



### Consultar espacio de cada nodo  

```sql
psql -p 55164 -c "SELECT pg_size_pretty(pg_database_size('postgres'));" # Coordinador  tamaÃ±o 9748 kB 
psql -p 55165 -c "SELECT pg_size_pretty(pg_database_size('postgres'));" # Worker1 tamaÃ±o  111 GB sin tabla columnar a 82 GB con tabla columnar 
psql -p 55165 -c "SELECT pg_size_pretty(pg_database_size('postgres'));" # Worker1  tamaÃ±o 111 GB
psql -p 55165 -c "SELECT pg_size_pretty(pg_database_size('postgres'));"  # Worker1  tamaÃ±o 111 GB

du -lh /sysx/data16/DATANEW/ | grep "/sysx/data16/DATANEW/$"

SELECT * FROM citus_tables;

```


### Consultas de prueba para medir el rendiemiento;
Esta consulta se puede realizar en cualquier nodo coordinador o worker, los resultados debe variar en los 20 o 30ms
```sql

select * from users where username in('user_558121771', 'user_546225936', 'user_104260803', 'user_435382393', 'user_436477677', 'user_480562912', 'user_220547089', 'user_465849719', 'user_922686500', 'user_67022343', 'user_864700256', 'user_96992106', 'user_97821595', 'user_804278165', 'user_634866347', 'user_910340103', 'user_797078547', 'user_252314339', 'user_127316864', 'user_255343492', 'user_32945176', 'user_397205450', 'user_922289404', 'user_989248229', 'user_95656', 'user_674104665', 'user_926157642', 'user_759415215', 'user_343356717', 'user_746892625', 'user_296891570', 'user_952883649', 'user_552850535', 'user_765911606', 'user_811833978', 'user_249614931', 'user_760638493', 'user_77138948', 'user_816895258', 'user_152084701', 'user_870678589', 'user_767589769', 'user_735472977', 'user_579996699', 'user_633072027', 'user_313737799', 'user_79859741', 'user_377459834', 'user_430415774', 'user_62891351');

```


 


### Ver la distribucion del sharding
```sql
SELECT 'select * from ' || shard_name || '; -- Ejecutar en Server: ' || nodename || ' - Port: ' ||nodeport FROM citus_shards where shard_size != 16384;


-- Querys monitoreo
SELECT * FROM citus_schemas;
SELECT * FROM citus_tables;
SELECT p.nodename, p.nodeport,s.* FROM pg_dist_shard s JOIN pg_dist_shard_placement p ON s.shardid = p.shardid;
SELECT * FROM citus_shards;
SELECT * FROM citus_stat_statements  LIMIT 10;
SELECT * FROM citus_stat_activity;
SELECT * FROM pg_dist_partition;
SELECT * from pg_dist_placement;
```



### Borrar el laboratorio 
```sql
/usr/pgsql-16/bin/pg_ctl stop -D /sysx/data16/DATANEW/data_coordinador
/usr/pgsql-16/bin/pg_ctl stop -D /sysx/data16/DATANEW/data_worker1
/usr/pgsql-16/bin/pg_ctl stop -D /sysx/data16/DATANEW/data_worker2
/usr/pgsql-16/bin/pg_ctl stop -D /sysx/data16/DATANEW/data_worker3

rm -r  /sysx/data16/DATANEW/data_coordinador
rm -r  /sysx/data16/DATANEW/data_worker1
rm -r  /sysx/data16/DATANEW/data_worker2
rm -r  /sysx/data16/DATANEW/data_worker3
```

---

 
###   Â¿CuÃ¡l es el objetivo de una tabla de referencia?

Las **tablas de referencia** se usan para **evitar la redistribuciÃ³n de datos** durante los `JOINs` en consultas distribuidas. Se replican en **todos los nodos del clÃºster**, lo que permite que cada nodo tenga acceso local a esa informaciÃ³n sin necesidad de hacer consultas remotas.


###   Ejemplo prÃ¡ctico

SupÃ³n que tienes:

- Una tabla distribuida: `ventas(distribuida por id_cliente)`
- Una tabla pequeÃ±a: `paises(id, nombre)`

Si haces muchas consultas como:

```sql
SELECT v.id, p.nombre
FROM ventas v
JOIN paises p ON v.pais_id = p.id;
```


###  Flujo de ejecuciÃ³n con tabla de referencia

1. El coordinador recibe la consulta.
2. Detecta que `ventas` estÃ¡ distribuida y `paises` es una tabla de referencia.
3. EnvÃ­a la consulta a cada nodo **junto con la lÃ³gica del `JOIN`**.
4. Cada nodo ejecuta la consulta **localmente**, porque ya tiene una copia de `paises`.
5. Los resultados se agregan y devuelven al cliente.


###   Â¿QuÃ© pasa si no usas tabla de referencia?

Si `paises` no es una tabla de referencia:

- El sistema tendrÃ­a que **redistribuir los datos** de `paises` o de `ventas` para hacer el `JOIN`, lo cual es **costoso y lento**.
- PodrÃ­as tener errores si los datos no estÃ¡n disponibles en todos los nodos.


 
###   2. Â¿Se pueden usar claves forÃ¡neas (foreign keys) en Citus?

**No directamente entre tablas distribuidas y de referencia.** PostgreSQL permite claves forÃ¡neas, pero **Citus no las aplica ni las valida automÃ¡ticamente** en entornos distribuidos. AquÃ­ los detalles:

| Tipo de tabla | Â¿Soporta claves forÃ¡neas? | Comentario |
|---------------|----------------------------|------------|
| Local         | âœ… SÃ­                      | Comportamiento normal de PostgreSQL |
| Distribuida   | âš ï¸ No                      | Citus ignora las `FOREIGN KEY` por rendimiento y escalabilidad |
| Referencia    | âš ï¸ Parcialmente            | Puedes definirlas, pero **no se aplican ni validan automÃ¡ticamente** |

> Puedes definir la clave forÃ¡nea para documentaciÃ³n o herramientas de modelado, pero **Citus no la harÃ¡ cumplir**.

---

###  Alternativas recomendadas

- Validar integridad referencial **a nivel de aplicaciÃ³n**.
- Usar **triggers** si necesitas validaciÃ³n estricta (aunque puede impactar el rendimiento).
- Si ambas tablas son de referencia, puedes usar claves forÃ¡neas normalmente.

 
 
---


## 4) Ejemplo de **`create_time_partitions`** (particiÃ³n por tiempo sobre una tabla distribuida)

Este patrÃ³n (Hash + time range partitioning) es **el recomendado** para logs/eventos/IoT/analÃ­tica con retenciÃ³n: te permite **indices pequeÃ±os**, **pruning** efectivo y **drops** de particiones antiguas. [\[citusdata.com\]](https://www.citusdata.com/blog/2023/08/04/understanding-partitioning-and-sharding-in-postgres-and-citus/), [\[github.com\]](https://github.com/mwendwa5/postgres-citus)

```sql
-- 1. Partimos de la tabla distribuida por HASH (por entidad) 
--    que definimos antes: public.events (distributed on tenant_id)

-- 2. Creamos particiones mensuales para los prÃ³ximos 12 meses
SELECT create_time_partitions(
  table_name         := 'public.events',
  partition_interval := '1 month',
  end_at             := now() + interval '12 months'
);

-- 3. (Opcional) Ver las particiones generadas por Citus
SELECT partition
FROM time_partitions
WHERE parent_table = 'public.events'::regclass;

-- 4. (Operativa) Con pg_cron puedes:
--    - Asegurar que siempre existan N particiones futuras
--    - Dropear particiones antiguas para retenciÃ³n
-- Ejemplo conceptual:
-- SELECT run_command_on_schedule(
--   job_name := 'events_partitions_maintenance',
--   schedule := '0 2 * * *',  -- diario a las 02:00
--   command  := $$ 
--     SELECT ensure_time_partitions(
--       'public.events', '1 month', now(), now() + interval '12 months'
--     );
--     SELECT drop_old_time_partitions(
--       'public.events', older_than := now() - interval '18 months'
--     );
--   $$ 
-- );
```


 
### âœ… Â¿QuÃ© significa *multiâ€‘tenant*?

*   **Tenant** = â€œinquilinoâ€ o â€œclienteâ€ en un sistema compartido.
*   **Multiâ€‘tenant** = una sola aplicaciÃ³n/base de datos sirve a **muchos clientes** (tenants), pero cada uno ve **solo sus datos**.

# **Consideraciones, recomendaciones y buenas prÃ¡cticas con Citus**:


## âœ… **1. ElecciÃ³n de la columna de distribuciÃ³n**

*   **Distribuye por la columna que mÃ¡s usas en filtros y JOINs** (p. ej., `tenant_id` en multiâ€‘tenant o `device_id` en IoT).
*   Evita distribuir por columnas que cambian frecuentemente (updates costosos).
*   Si no hay una clave natural, considera una **clave sintÃ©tica**.



## âœ… **2. Coâ€‘location para JOINs**

*   Si varias tablas se consultan juntas, distribÃºyelas por la **misma columna** para que los JOINs ocurran en el mismo nodo.
*   Usa **reference tables** para catÃ¡logos pequeÃ±os (replicadas en todos los nodos).



## âœ… **3. Particionamiento por tiempo**

*   Combina **Hash distribution** con **particiones por tiempo** para:
    *   **Pruning** (consultas mÃ¡s rÃ¡pidas).
    *   **Drops instantÃ¡neos** para retenciÃ³n.
*   Usa `create_time_partitions()` y automatiza con **pg\_cron**.



## âœ… **4. Ãndices y mantenimiento**

*   Crea Ã­ndices en columnas de filtro (por shard/particiÃ³n).
*   Ajusta parÃ¡metros como `work_mem`, `maintenance_work_mem` para consultas distribuidas.
*   Usa **ANALYZE** regularmente para estadÃ­sticas correctas.



## âœ… **5. Evita scatterâ€‘gather innecesario**

*   Si no filtras por la columna de distribuciÃ³n, la consulta serÃ¡ **global** (scatterâ€‘gather).
*   Para analÃ­tica pesada, considera **vistas materializadas** o **tablas agregadas**.



## âœ… **6. Escalabilidad y rebalanceo**

*   Empieza con **1 coordinator + 3 workers** mÃ­nimo.
*   Usa `rebalance_table_shards()` cuando agregues nodos.
*   Planifica shards suficientes desde el inicio (ej. 32â€‘64 shards por tabla).



## âœ… **7. Alta disponibilidad**

*   Configura **replicaciÃ³n streaming** en cada worker.
*   Usa **Patroni** o similar para failover automÃ¡tico.



## âœ… **8. Monitoreo y alertas**

*   Monitorea:
    *   Latencia de consultas distribuidas.
    *   Estado de shards (`pg_dist_shard`).
    *   Espacio por nodo.
*   Herramientas: **pg\_stat\_statements**, Prometheus + Grafana.



## âœ… **9. Seguridad**

*   Roles y permisos centralizados en el **coordinator**.
*   Cifrado en trÃ¡nsito (SSL) y en reposo si aplica.
*   Limita acceso directo a workers (solo coordinator expuesto).
 

## âœ… **10. Actualizaciones y compatibilidad**

*   Usa versiones recientes de PostgreSQL y Citus (>= 12).
 

 

## Info extra 
```
postgres@postgres# \dx+ citus

postgres@postgres# select proname from pg_proc where proname ilike '%balan%';
+---------------------------------------------+
|                   proname                   |
+---------------------------------------------+
| citus_validate_rebalance_strategy_functions |
| citus_set_default_rebalance_strategy        |
| rebalance_table_shards                      |
| get_rebalance_table_shards_plan             |
| citus_rebalance_start                       |
| citus_rebalance_stop                        |
| citus_rebalance_wait                        |
| get_rebalance_progress                      |
| citus_rebalance_status                      |
| pg_dist_rebalance_strategy_trigger_func     |
| citus_add_rebalance_strategy                |

```

## BibliografÃ­a
```
https://docs.citusdata.com/en/v13.0/
https://docs.citusdata.com/en/v10.2/cloud/availability.html
https://www.citusdata.com/blog/2018/02/21/three-approaches-to-postgresql-replication/
https://github.com/citusdata/citus
Citus 12: Schema-based sharding for PostgreSQL -> https://www.citusdata.com/blog/2023/07/18/citus-12-schema-based-sharding-for-postgres/
Sharding Postgres on a single Citus node, how why & when -> https://www.citusdata.com/blog/2021/03/20/sharding-postgres-on-a-single-citus-node/
Citus 11.1 shards your Postgres tables without interruption -> https://www.citusdata.com/blog/2022/09/19/citus-11-1-shards-postgres-tables-without-interruption/

citus â€” distributed database and columnar storage functionality  -> https://postgrespro.com/docs/enterprise/16/citus.html
Postgres + Citus + Partman, Your IoT Database -> https://www.crunchydata.com/blog/postgres-citus-partman-your-iot-database
Citus: Sharding your first table -> https://www.cybertec-postgresql.com/en/citus-sharding-your-first-table/
Citus: The Misunderstood Postgres Extension -> https://www.crunchydata.com/blog/citus-the-misunderstood-postgres-extension


Scaling Horizontally on PostgreSQL: Citusâ€™s Impact on Database Architecture -> https://demirhuseyinn-94.medium.com/scaling-horizontally-on-postgresql-cituss-impact-on-database-architecture-295329c72c62
Insights from paper: Citus: Distributed PostgreSQL for Data-Intensive Applications -> https://hemantkgupta.medium.com/insights-from-paper-citus-distributed-postgresql-for-data-intensive-applications-6224a12af32d
Scaling PostgreSQL with Citus: A Practical Guide -> https://oluwatosin-amokeodo.medium.com/scaling-postgresql-with-citus-a-practical-guide-14d86c87ccfb
Debeziumâ€™s adventures with Citus -> https://robert-ganowski.medium.com/debeziums-adventures-with-citus-a5883cc60856
Sharding PostgreSQL with Citus and Golang -> https://medium.com/@bhadange.atharv/sharding-postgresql-with-citus-and-golang-on-gofiber-21a0ef5efb30
Multi-Node Setup using Citus -> https://medium.com/@bhadange.atharv/citus-multi-node-setup-69c900754da3
How to find table size in Citus? -> https://medium.com/@smaranraialt/table-size-in-citus-c1fb579159fb
Mastering PostgreSQL Scaling: A Tale of Sharding and Partitioning -> https://doronsegal.medium.com/scaling-postgres-dfd9c5e175e6
Scaling for millions: PostgreSQL -> https://medium.com/@sabawasim.it/scaling-for-millions-postgresql-4898acfb0abe
Data Redundancy With the PostgreSQL Citus Extension -> https://www.percona.com/blog/data-redundancy-with-the-postgresql-citus-extension/

PostgreSQL: 1 trillion rows in Citus -> https://www.cybertec-postgresql.com/en/postgresql-1-trillion-rows-in-citus/

Bases de datos distribuidas PostgreSQL al lÃ­mite -> https://www.youtube.com/watch?v=5SZVJgg94k4

Manejo de miles de millones de filas en PostgreSQL ( TimescaleDB ) -> https://medium.com/timescale/handling-billions-of-rows-in-postgresql-80d3bd24dabb

```
